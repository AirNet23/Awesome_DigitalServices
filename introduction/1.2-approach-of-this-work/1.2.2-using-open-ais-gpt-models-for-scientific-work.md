# 1.2.2 Using Open AI’s GPT models for scientific work

I decided to use GPT-3 generated text first in addition to answer research questions on social media services and later GPT-4 / Chat-GPT. The models can be accessed by the OpenAI Playground from the company OpenAI. While Chat-Gpts model gpt-turbo 3.5 is free GPT-3 and GPT-4 are both based on a per token payment model.

_See:_ [_https://platform.openai.com/docs/models_](https://platform.openai.com/docs/models)_, accessed on 10.04.2023_



**1.2.2.1 Violation of scientific principles**

To generate informal text by LLMs and use it later for scientific work can possible lead to a lot of research procedure erorrs and spreading of false information. This happens because scientific experience have shown, that scientific work and texts need to fulfill a number of properties (sections I. and II.) to provide the base for further research work. The following list is an attempt to list all violations of properties (subsections I.I etc..) I could discover.



**I Repeatability of scientific works**

**I.I Input parameter changes affecting LLM output**

Based on the same given input this models starts to predict the result token by token partly randomly. A token is a combination of letters, mostly from 2 to 4 letters like („dog“, „au“, „etc“) that will form a word, or a part of a word or just a gramatically sign („!“). They are formed automatically by the process that is called tokenization on probability of occuring.

For each next predicting token it produces an list of probabilities for all possible tokens the system knows. An sampling algorithm now decides which token to choose. An tactic would be to use that one with highest probability. Another one would be sampling from the 3 tokens with the highest probability or downrank reoccuring words. There are many more advanced impleemnted in the current models. Different approaches are used to prevent the language model from producing the same results again (especially looping) and adapt its output to different use cases.

To adapt this sampling algorithm OpenAI’s Api allows me accces to the **temperature** parameter and the **top\_p** parameter.

See: [_https://platform.openai.com/docs/api-reference/completions/create_](https://platform.openai.com/docs/api-reference/completions/create), accessed on 10.04.2023

**Temperature:**

The temperature parameter is an analogy to thermodynamics, „(…) where high temperature means low energy states are more likely encountered.“ Mapping this on tokens high temperature means that tokens with low probability will choosen more often (choosen) by the sampling algorithm. Low temperature means that tokens with low probability will be choosen almost never.

This behaviour is often implemented by recalculating the propabilities for the next tokens with a function that takes in regard the temperature parameter. (see the „article sampling“). Figure 1 describes this transformation by plotting the start probabilities on the x-axis and the resulting probabilities on the y-axis.

After that the models sampling algorithm still using the new probabilities to choose a word implemented by another meachanism. e.g. „top\_p sampling“.



<figure><img src="../../.gitbook/assets/0 (3).png" alt=""><figcaption><p>Figure 1: x-axis: previous token probability; y-axis: probability after transformation</p></figcaption></figure>

See: Article „Sampling“ cf. [_https://towardsdatascience.com/how-to-sample-from-language-models-682bceb97277?gi=0d4b4342fe67_](https://towardsdatascience.com/how-to-sample-from-language-models-682bceb97277?gi=0d4b4342fe67)

In practise, when choosing temperature the answer should differ in the actual choosen words and focus areas for the answer. I could find some standard reccomondations on temperature solving different task with language models:

| Parameter Interval | Task                                                                                                       | Behaviour                                                                                                               |
| ------------------ | ---------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------- |
| temp = 1           | The authors couldnt name a good use-case.                                                                  | Very creative inconsistent indeterministic results, risks of made up or factual false content (called „hallucinations“) |
| 0.9 <= temp < 1    | No information given. Perhaps: Open ended creative tasks, jokes                                            | No information given. On a scale between indeterminisitc and deterministic.                                             |
| 0.7 <= temp < 0.9  | Creative tasks: Story writing, idea generation, marketing, advertising, generate many variations of a text | No information given. On a scale between indeterminisitc and deterministic.                                             |
| 0.3 <= temp < 0.7  | <p>Other writing taks.</p><p>Perhaps: Argumentation, commentary, essay</p>                                 | No information given. On a scale between indeterminisitc and deterministic.                                             |
| 0 < temp < 0.3     | Transformation tasks: extraction, standardization, format conversion, grammar fixes                        | No information given. On a scale between indeterminisitc and deterministic.                                             |
| Temp = 0           | Factual output, classification                                                                             | Stable, mostly deterministic output                                                                                     |

Table 1. Reccomondations of choosing the temperature parameter

See: [_https://algowriting.medium.com/gpt-3-temperature-setting-101-41200ff0d0be_](https://algowriting.medium.com/gpt-3-temperature-setting-101-41200ff0d0be), accessed on 10.04.2023

See: [_https://gptforwork.com/guides/openai-gpt3-temperature_](https://gptforwork.com/guides/openai-gpt3-temperature), accessed on 10.04.2023

**Top\_p**

Open-Ais GPT models using a technique called „Nuclues Sampling“ to determine the actual token to output of the list of proabilities for tokens (sampling).

_Here's how the selection in nucleus sampling works step by step:_

1\. Generate the list of probabilties by using the model

2\. Nucleus sampling can be done by sorting the token probability list from high to low and than starting to sum up beginning from the tokens with highest probabilties. The laws of statics gurrantee us that summing up all token probabilities will give us always one.

3\. By adding the probabilities of more tokens continiously our sum will grow and we can add that token to our **list of possible tokens (nucleus)** for the next step. Our **top\_p** parameter sets now the upper limit for the sum. (cummulative\_p <= top\_p) When the sum is creater or equal to it we will stop adding tokens to the \*\*list of possible tokens. \*\*

4\. Now we can use this list of possible tokens to do a random selection of the next token on it.

_I used ChatGPT to put it in more technical terms:_

<mark style="color:blue;">**Q: \[not documented while writing this text]**</mark>

<mark style="color:blue;">**A:**</mark>

1. <mark style="color:blue;">First, the model calculates the probability distribution over the entire vocabulary for the next word, based on the current input and the model's parameters.</mark>
2. <mark style="color:blue;">Then, the model sorts the vocabulary in descending order of probability, and calculates the cumulative probability of the sorted words.</mark>
3. <mark style="color:blue;">Nucleus sampling selects the smallest possible set of words that account for a given proportion p (e.g., p = 0.9 or p = 0.5) → (</mark><mark style="color:blue;">**Top\_p)**</mark> <mark style="color:blue;"></mark><mark style="color:blue;">of the total probability mass. This set of words is called the "nucleus".</mark>
4. <mark style="color:blue;">Finally, the next word is randomly sampled from the words in the nucleus, with each word being assigned a probability proportional to its original probability in the distribution.</mark>

Source: Generated by open-AI Chat-GPT-turbo-3.4 on 10.04.2023

| Phrase                | Definition                                                                                                                                                                                                                                           |
| --------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| _Broad Distribution_  | There are many outcomes with near average (low) probabilities in a distribution. In this case: The model predicts many different tokens with low probabilities in the tokens probabilities list. (<mark style="color:orange;">orange</mark>)         |
| _Narrow distribution_ | There are some outcomes with high probability and many with low probabilities in a distribution. In this case: The model predicts many different tokens with some tokens having very high probabilities. (<mark style="color:purple;">purple</mark>) |

When setting top\_p to 0.9 on the Narrow distribution (purple). It will only add „hot“ and „warm“ to the **list of possible tokens**, because together they have a sum of around 0.9 (cummulative probability).

Doing the same on the Broad distribution (<mark style="color:orange;">orange</mark>) will give us a list with many entries in the **list of possible tokens,** because it takes many words until there probabilties add up to 0.9.

The same top\_p can result on **list of possible tokens** of various sizes regarding to the underlaying distribution. But it will gurantee that always the most probable variants are selected.



<figure><img src="../../.gitbook/assets/1 (2).png" alt=""><figcaption><p>Figure 2. yellow: Probabilties of tokens in the user input „She said, „ I never “ with list of probabilities for possible next token and a vertical line which symbolizing a max probability token probabilities of 0.08. purple: Probabilties of tokens in the user input „I ate the pizza while it was still“ with list of probabilities for possible next token and a vertical line which symbolizing a max probability over all token probabilities of 0.8.</p></figcaption></figure>

See: [_https://towardsdatascience.com/how-to-sample-from-language-models-682bceb97277?gi=0d4b4342fe67_](https://towardsdatascience.com/how-to-sample-from-language-models-682bceb97277?gi=0d4b4342fe67)

I couldn’t discover a guide which top\_p values are pratical. But one author could get **optimal result by a mean top\_p of 0.59** trying out multiple top\_p’s on a wikipedia text dataset. In general a **high top\_p** will result to a **higher creativity and indeterminsitic answers** and a **low top\_p to deterministic answer.**

_**See:**_ [_https://towardsdatascience.com/how-to-sample-from-language-models-682bceb97277?gi=0d4b4342fe67_](https://towardsdatascience.com/how-to-sample-from-language-models-682bceb97277?gi=0d4b4342fe67)_**, accessed on 10.04.2023**_

Open AI recommends **altering „top\_p“ or „temperature“ but not both.** Because it could lead to unpredictable behaviour, since the parameters effect the processes of each other. While low temperature would lead to an increase of probability for tokens of already high probabilites the top\_p will create a smaller nuclues. But with a higher top\_p at the same time the effects of low temperature could be canceled out. The sampling algorithm will react differently if temperature and top\_p are applied in reverse order.

See: [_https://platform.openai.com/docs/api-reference/chat_](https://platform.openai.com/docs/api-reference/chat)

OpenAi provides further parameters to tune the samplign algorithm named ‚presence\_penalty‘, ‚frequency\_penalty‘ and ‚logit\_bias‘. While the former are used to reduce the probability of already present tokens generated again. The latter is used to apply a bias by the user (increase or decrease probability) for specific tokens.

See: [_https://platform.openai.com/docs/api-reference/parameter-details_](https://platform.openai.com/docs/api-reference/parameter-details), accesed on 10.04.2023

**I.II The Open AI provided model is changed regulary**

OpenAI can change the used models for their service. For example they changed from model chat-gpt to chat-gpt.3.5-turbo for their product ChatGPT, which will produce other results.

They write „With the release of gpt-3.5-turbo, some of our models are now being continually updated. In order to mitigate the chance of model changes affecting our users in an unexpected way, we also offer model versions that will stay static for 3 month periods.“

See: „[_https://platform.openai.com/docs/models/gpt-3_](https://platform.openai.com/docs/models/gpt-3)“.

This is a problem since we wont have any further acccess on these after publisihing this article…

**I.III Rephrasing a prompt about the same topic changes the output**

Putting the same request to the model in different words, even slightly varying can lead to totally differrent list of probabilities for the next token, since some phrases having double meanings.



**II Truthfullness**

**II.I Model hallucinaton**

The LLM could output text, which information are false or made up. This seems to happen, when there were no much information regarding to a questions available in the training data.



**II.II Deprecated information**

The models are trained in the past with data from the internet. After the training process a relevant part of their structure is not changed anymore. Some models are getting aligned by human team to response in a more polite and harmless way later, but this doesn't change their core struture (as far as I know).

For example for ChatGPTs training data reached until September 2021, so its fact based knowledge cout off at this date.

See: [_https://ai.stackexchange.com/questions/39686/chatgpt-is-aware-of-todays-date_](https://ai.stackexchange.com/questions/39686/chatgpt-is-aware-of-todays-date) accessed on 19.05.2023



**III. Traceability**

**III.I Own thoughts and LLM generated thought chains are fading seamlessly**

While reading LLM generated texts to a topic your brain will learning new information. In some cases I will reproduce this information subconsciously.

I also suppose since the associative nature of the brain and the property of LLMs to generate the statistically possibles answer and the nature of the training data (human texts as result of brains thought processes) the change of learning true and false facts by LLMs texts is even higher. (This paragraph is an unconfirmed assumption by me)

For further works I could also include these text sippets generated by a LLM consciously. Its very hard for a human to attribute these textes to a human thought process of me or to the function of an LLM.

The attribution to LLM or human is important, since the generation of thoughts following different patterns and this having consequence to the quality of the textes. (This paragraph is an unconfirmed assumption by me)

**III. II The LLM generated information are not aligned to the user prompt**

Even if natural language can be used now to express a wide range of request, some specific prompts get misunderstood. This happens mostly because of missing contect or unspecific use of technical words.&#x20;

In consequence the prompt need to be repeated. This leads to longer research time and costs produced by model usage.

**III. III Topic looping**

Another observation is that the LLM starts to repeat already communciated information in its answer by repeating whole word groups or paraphrasing. This seem also to occure if there was not much training data or the prompt is very general. This phenomen is treated as violation because it complicates to understand the authors thoughts.



**1.2.2.2 Measurements**

The following is a try to tackle each observed violation with some specific measurements. The most important one is to develope a standardized way to quote LLM generated text and write down the source reference.

**I recommend a syntax for source references:**

Source \[source shortcode], model(temp= 'value',top\_p= 'value'), short description or URL how to access it, valid till: 'training data cut off date', date

e.g. Source \[RZT], gpt-3.5-turbo-03-01(temp = 0.7,top\_p=1), valid till: 09.2021, 15.01.2023

**I recommend a syntax and formatting for quoting generated text:**

AI generated quoted text will be marked **blue** in this work.

The user input prompt is marked with a leading **Q:** at the model generated text and the answer is marked with **GPT-3:** or **A:**

If the LLM was instructed to provide sources for a statement:

* If the source is searchable and information are correct I will put **\[Source okay]** behind these source and leave the text segemnt in default formatting written as above.
* If the source is not reachable, not existing or contains false information I will put **\[Source 'error description']** behind these source and format the text to **subscript.**

**I recommend a syntax for providing details to models in the bibliography of an article / text:**

Model name, 'list of standard parameters', short description or URL how to access it, cut off date of the training data, model distributor / developer

e.g gpt-3.5-turbo-03-01, \[temperature=1, maximum\_length=256, top\_p=1,frequency\_penalty=0, presence\_penalty=0] [_https://platform.openai.com/playground_](https://platform.openai.com/playground), valid till: 09.2021, OpenAI

**Here is a comparison of how this contributes to the containment of the violations:**

| Violation                                                                           | Measurement                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
| ----------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| I.I Input parameter changes affecting LLM output                                    | <p>All parameters values deviating from the model standard values will be added in the source reference.</p><p>... model(temp='value',top_p='value')</p><p>Warning: I am unsure about the actual sampling algorithm for GPT-4 and gpt 3.5-turbo since top_p should make temperature effectless, but its still effective. So to deterermine the next generated token there couldn’t be used a random selection from the nucleus.</p>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
| I.II The Open AI provided model is changed regulary                                 | The used model version will be added in the source reference of generated text using the following syntax:                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |
| <p><br></p>                                                                         | <p>...model(temp='value',top_p='value')</p><p>Hint: The model version can be passed in the api or adapted with sliders in the OpenAi playground. I will use gpt-4-0314, gpt-4-32k-0314 and gpt-3.5-turbo-0301 for this work.</p>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
| I.III Rephrasing a prompt about the same topic changes the output                   | I will deposit the exact phrasing of the user input /prompt behind a leading Q: at the model generated text. The answer is marked with GPT-3: or A:                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
| II.I Model hallucinaton                                                             | <p>When asking the model for factual information I will instruct it to provide sources to each of its statements by sending the following system prompt:</p><p><em>“Based on the information and sources you have encountered during your training, please provide a detailed response to the user's request, ensuring that you cite the sources correctly using indirect quotes. In the citation you have to include the title of the source and a method for the user to locate the referenced information. Do not invent sources or present false information. Prefer books, scientific journals and articles as sources.”</em></p><p></p><p><strong>[III. Test of providing sources via LLM]</strong> contains an demonstration of this prompt.</p><p>I will recheck manually if the provided sources are existing and provide correct information and change formating of the segment as described above.</p><p><mark style="color:red;">Warning: The cited sources by the LLM are often invalid. In total the prompt contribute to the solution of the problem only to a low amount. I will decide if the effort of rechecking sources is in relative to the usefullness of the answer for every prompt.</mark></p> |
| II.II Deprecated information                                                        | <p>Goal is to improve the perception of the trainings data cut off date of the reader. He should be able to decide how he evualtes the information in a generated text. To assess the validity of statements from a scientific work for today I reccommend to provide the cut off date in the source reference:</p><p>gpt-3.5-turbo-03-01(temp = 0.7,top_p=1), valid till: 09.2021, 15.01.2023</p><p>I need to do my own research if I want to consider newer information than the training data of the LLM or change the LLM.</p>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| III.I Own thoughts and LLM generated thought chains flow seamlessly into each other | <p>I will mark texts that are generated by GPT-models blue in this work.</p><p>I also recommend to provide every LLM generated text I requested and read while doing research at the appendix. This will help to narrow down conscious or subconscious incluences of LLM texts on my thoughts and writings.</p>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
| III. II The LLM generated information are not aligned to the user prompt            | <p>In general a prompt should be prepared very thoughtful by splitting a task into subtasks and using clear and error free language.<br>If a prompt is not working: There are various writing techniques and algorithms to optimize the user prompts to the LLM, so it get answered more aligned. For example a comprehensive guide can be found at: <a href="https://www.promptingguide.ai/">https://www.promptingguide.ai/</a><br></p>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
| III. III Topic looping                                                              | <p>Further research needed. I couldn't found any recommondation how to prevent it. <br>The model GPT-3/GPT-4 offering the 'frequency_penalty' parameter that should reduce this problem, but I didn't test it yet.</p>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
|                                                                                     |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
